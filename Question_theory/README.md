# Q. 理論編

ここではディープラーニングの理論をnumpyを使って自分の手で実装していきます。わからないと無限に泥沼にはまってしまいやすいので、ちょっと答えを見たりしてコーディングしてみましょう。

## Q. パーセプトロン Step.1

パーセプトロンは人の脳にある神経細胞、ニューロンを数式モデリングしたものです。その仕組は入力値それぞれに重みというパラメータを乗算し、それを加算して出力するシンプルなものです。

![](assets/perceptron.jpg)

ｂは**バイアス**と呼ばれ、ただ、数値を加算するだけの仕組みです。

なので、入力が[x1, x2], 重みが[w1, w2], y = w1 x1 + w2 x2 + b という式ですが、bを w3 x 1 という風に見てやると、入力1に重みw3をかけたものと見なせます。
よって、入力が[x1, x2, 1] , 重みが[w1, w2, w3],  y = w1 x1 + w2 x2 + w3 x3 という風に変換することができます。

ここではパーセプトロンで下記のクラス分類を解きます。これはANDゲートです。まずはこれを実装しましょう。

| 入力 | 出力（クラス）|
|:---:|:---:|
| [0, 0] | -1 |
| [0, 1] | -1 |
| [1, 0] | -1 |
| [1, 1] | 1 |

これはこんな風に書きます。

```python
x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)
t = np.array([[-1], [-1], [-1], [1]], dtype=np.float32)
```
重みはこれを用いて下さい。

```python
import numpy as np
np.random.seed(0)
w = np.random.normal(0., 1, (3))
```

次に、入力にバイアス用の[1]を追加して、それぞれ y の値を計算してみましょう。

こういう結果になると思います。

```bash
weight >> [1.76405235 0.40015721 0.97873798]
in >> [0. 0. 1.] y >> 0.9787379841057392
in >> [0. 1. 1.] y >> 1.3788951924729624
in >> [1. 0. 1.] y >> 2.742790330073403
in >> [1. 1. 1.] y >> 3.1429475384406267
```

答え >> [answers/perceptron1.py]( https://github.com/yoyoyo-yo/DeepLearningMugenKnock/blob/master/Question_theory/perceptron1.py )

## Q. パーセプトロン Step.2

出力が得られたら、次に学習を行っていきます。
そもそも学習というのは**パラメータの自動更新**をいいます。ここでいうパラメータとは重みのことです（バイアスもめんどいので重みとして扱います）。

重みをどうやって更新するかというと、出力を見るわけです。今回の問題は最後の出力が-1か1なので２クラス分類という問題に回帰されます。ですが、先程の出力を見ると、0.9787とかになってます。こういうときは、出力が0以上を1、0未満を-1という風に考えて、問題にフィットさせます。（こういう操作は**活性化関数**と呼ばれるもので操作されていきますが、これは後々）

パーセプトロンでは、出力が逆だった場合、En = ラベル x 入力値　 を重みの更新料とします。（Enは**誤差関数**と呼ばれます。）
つまり、重み w に対して、w = w - lr En を計算します。 lrは**学習率**といい、更新の度合いを表します。0.1や0.01など[0,1]の範囲を取ります。

これを全てのデータに対して更新がなくなるまで、延々と繰り返します。これが学習です。ディープラーニングはこの学習をめちゃくちゃやってすごい成果を出しています。

学習のアルゴリズムは、
1. ４つの入力xに対する出力yを計算
2. yが逆のクラスを指していたら、 En = 正しいラベル x 入力値　を計算
3. w = w - lr En を計算する
4. 2と3を全てのデータで更新がされなくなるまで、４つのデータで繰り返す。
ここではlr=0.1として、これを学習を行ってみましょう。

最終的にこんな風になると思います。iterationというのは４つのデータで一回学習させた、という意味です。つまり、13周して出力が望み通りになったということです。iterationの右の4つの数値はyの出力です。

重みが学習前後で更新されているのが分かるが、なにが起こったかというと、 w1 x1 + w2 x2 + w3 1 において、入力[1, 1]のみが出力1 (つまり0以上)となる重みになりました。weightの3番目の数値がマイナスになっているが、つまり入力[1,1]に対してw1 + w2 + x3がギリギリ0以上になる重みになったことになります。これを**最適化**といいます。

```bash
weight >> [1.76405235 0.40015721 0.97873798]
iteration: 1 y >> [0.97873798 1.37889519 2.74279033 3.14294754]
iteration: 2 y >> [0.67873797 0.97889518 2.34279032 2.64294752]
iteration: 3 y >> [0.37873796 0.57889517 1.9427903  2.14294751]
iteration: 4 y >> [0.07873795 0.17889515 1.54279029 1.64294749]
iteration: 5 y >> [-0.22126206 -0.22110486  1.14279028  1.14294748]
iteration: 6 y >> [-0.32126207 -0.32110486  0.94279027  0.94294748]
iteration: 7 y >> [-0.42126207 -0.42110486  0.74279027  0.74294747]
iteration: 8 y >> [-0.52126207 -0.52110487  0.54279027  0.54294747]
iteration: 9 y >> [-0.62126207 -0.62110487  0.34279026  0.34294747]
iteration: 10 y >> [-0.72126207 -0.72110487  0.14279026  0.14294746]
iteration: 11 y >> [-0.82126207 -0.82110487 -0.05720974 -0.05705254]
iteration: 12 y >> [-0.72126207 -0.62110487  0.14279026  0.24294747]
iteration: 13 y >> [-0.82126207 -0.72110487 -0.05720974  0.04294746]
training finished!
weight >> [ 0.76405233  0.1001572  -0.82126207]
in >> [0. 0. 1.] , out >> -0.8212620725186733
in >> [0. 1. 1.] , out >> -0.7211048686217985
in >> [1. 0. 1.] , out >> -0.057209741452170504
in >> [1. 1. 1.] , out >> 0.04294746244470449
```

答え >> [answers/perceptron2.py]( https://github.com/yoyoyo-yo/DeepLearningMugenKnock/blob/master/Question_theory/perceptron2.py )

## Q. パーセプトロン Step.3

lr=0.1と0.01で収束までの、w1, w2, w3の値をそれぞれ線グラフでプロットしてください。

![](answers/answer_perceptron3.png)

これを見ると、lr=0.1の方が早く学習を終わっていることが分かる。いわゆる学習が終了したことは、最適化によって最適解に到達したことを意味する。lr=0.01ではゆっくりではあるが、最適解に向かっていることが分かる。lr=0.1では学習が終了する直前に値が増えている。解は"谷"の底であり、学習とは谷の底に向かっていくが、学習率が大きいと底に向かう速度が早いため通り越してしまうこともある。これが原因で値が一瞬上昇した。（ただし谷に向かうのは値が減少する方向とは限らない。今回は重みの初期値の関係でたまたま減少の方向だった）


答え >> [answers/perceptron3.py]( https://github.com/yoyoyo-yo/DeepLearningMugenKnock/blob/master/Question_theory/perceptron3.py )
