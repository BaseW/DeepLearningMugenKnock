{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WJM5vNrDyIHSMIyxLVNbPvuYSI91jaZX",
      "authorship_tag": "ABX9TyMwqcVWHOqMDSTAu02C+ibA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyoyo-yo/DeepLearningMugenKnock/blob/master/pytorch/Word2vec_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcYMnxbsa_l",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHNQSDFxhp_",
        "colab_type": "text"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ywkt2AxjCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG65jE34sX7Y",
        "colab_type": "text"
      },
      "source": [
        "# Ginza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5b0-R8Ase_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "caa12fde-bd4e-4140-f698-1fc0065a53ce"
      },
      "source": [
        "!pip install ginza"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ginza\n",
            "  Downloading https://files.pythonhosted.org/packages/93/43/43818861210c71a0a9f789e7350b785ba60bad38196745c2f8e88271dfa8/ginza-3.1.2.tar.gz\n",
            "Requirement already satisfied: spacy>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from ginza) (2.2.4)\n",
            "Collecting ja_ginza<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/02/ad08f43df50c5d877a7b2dcd56f4ebf33a5818b517550516e8ba059069fa/ja_ginza-3.1.0.tar.gz (54.9MB)\n",
            "\u001b[K     |████████████████████████████████| 54.9MB 60kB/s \n",
            "\u001b[?25hCollecting SudachiPy>=0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/91/48412ffb70136fd7ab8f9fe4fc897653cec2590aa29baa762ef9f4ab4731/SudachiPy-0.4.6.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (47.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (4.41.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (2.23.0)\n",
            "Collecting ja_ginza_dict<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/e5/c4ea3f165acb38fa125e992d1a6fcfb9a5a8ab6a14cc5010c836713c386b/ja_ginza_dict-3.1.0-1.tar.gz (44.8MB)\n",
            "\u001b[K     |████████████████████████████████| 44.8MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers~=2.1.0 in /usr/local/lib/python3.6/dist-packages (from SudachiPy>=0.4.2->ginza) (2.1.0)\n",
            "Collecting dartsclone~=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/34/987a076369ed086ee953e2f0b9ab5ff3e1a682ba4f781678ac5648144896/dartsclone-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (474kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.3->ginza) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (3.0.4)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->SudachiPy>=0.4.2->ginza) (0.29.19)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.3->ginza) (3.1.0)\n",
            "Building wheels for collected packages: ginza, ja-ginza, SudachiPy, ja-ginza-dict\n",
            "  Building wheel for ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ginza: filename=ginza-3.1.2-cp36-none-any.whl size=17311 sha256=2047b11dad29d16b2b3baaa0b487ba4d0e1d2c6c1142b83e9c95fe645770b84f\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/8d/57/f089078acc0dbaebffc08c178e9f20924fa794c114ad36f7f7\n",
            "  Building wheel for ja-ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ja-ginza: filename=ja_ginza-3.1.0-cp36-none-any.whl size=54963619 sha256=fba8d6df290d93811d048526ff44b2ed591798d5ce0451316da2a28c6311dbbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/8a/07/1837eeb5c5648fa8d266102b78a894e495234585ac3f024cf1\n",
            "  Building wheel for SudachiPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiPy: filename=SudachiPy-0.4.6-cp36-cp36m-linux_x86_64.whl size=865416 sha256=74cc74fbc844734aa46e174e0ef3345a59bbecf65945b022139e39f084413234\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/80/5f/cc3ed568c44648e51b5984f78fa0c5015af40bd1cadcd68fac\n",
            "  Building wheel for ja-ginza-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ja-ginza-dict: filename=ja_ginza_dict-3.1.0-cp36-none-any.whl size=70877544 sha256=309a4ffc20d3b22b3af528a4fb117ffb92faf567d8f5ea39e701374bf574b436\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/88/d7/7f0692ba26060966af34538e1079438d16640c54e04a15a76a\n",
            "Successfully built ginza ja-ginza SudachiPy ja-ginza-dict\n",
            "Installing collected packages: ja-ginza-dict, ja-ginza, dartsclone, SudachiPy, ginza\n",
            "Successfully installed SudachiPy-0.4.6 dartsclone-0.9.0 ginza-3.1.2 ja-ginza-3.1.0 ja-ginza-dict-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXHHMNq7tN6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkg_resources, imp\n",
        "imp.reload(pkg_resources)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9G9Xg7Ms_Zu",
        "colab_type": "text"
      },
      "source": [
        "# Get corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV6jrnwKshi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus(fname):\n",
        "    corpus = []\n",
        "\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _corpus = []\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    _corpus.append(token.orth_)\n",
        "\n",
        "            corpus = list(set(corpus) | set(_corpus))\n",
        "    corpus.sort()\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxNN4BkHshgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample\n",
        "corpus = get_corpus('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt')\n",
        "corpus = ['<UNKNOWN>'] + corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPK7BsVt7p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    _corpus = get_corpus(fpath)\n",
        "    corpus = list(set(corpus) | set(_corpus))\n",
        "\n",
        "corpus.sort()\n",
        "\n",
        "corpus = ['<UNKNOWN>'] + corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omOOGN9y1TJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6c28e92b-bd3d-493d-c857-19ad4c47dd06"
      },
      "source": [
        "# show sample\n",
        "corpus[:20]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNKNOWN>',\n",
              " '!',\n",
              " '(',\n",
              " ')',\n",
              " '.co.jp',\n",
              " '1',\n",
              " '10',\n",
              " '100',\n",
              " '10万',\n",
              " '110',\n",
              " '12',\n",
              " '146',\n",
              " '2',\n",
              " '20',\n",
              " '200',\n",
              " '2007',\n",
              " '2008',\n",
              " '2009',\n",
              " '2010',\n",
              " '211']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJR93YOD1mjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3843caf5-f6d1-4353-ca51-ce413e07c4fe"
      },
      "source": [
        "# corpus len\n",
        "len(corpus)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5MAviutx16",
        "colab_type": "text"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A41KZI6dtTYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(fname, corpus):\n",
        "    Xs = []\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _Xs = [corpus.index('<SOS>')]\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    w = token.orth_\n",
        "\n",
        "                    if w in corpus:\n",
        "                        ind = corpus.index(w)\n",
        "                    else:\n",
        "                        ind = corpus.index('<UNKNOWN>')\n",
        "                    _Xs.append(ind)\n",
        "            _Xs.append(corpus.index('<EOS>'))\n",
        "            Xs.append(_Xs)\n",
        "\n",
        "    return np.array(Xs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlqoowV0Bpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(fname, corpus):\n",
        "    Xs = []\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read().rstrip()\n",
        "\n",
        "    Xs = [corpus.index(token.orth_) if token.orth_ in corpus else corpus.index('<UNKNOWN>') for sent in nlp(data).sents for token in sent]\n",
        "\n",
        "    return np.array(Xs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztiXC6Y0sVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read_data('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt', corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRprBc1sshda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data, w2v_c=None):\n",
        "    Xs = []\n",
        "    ts = []\n",
        "\n",
        "    w2v_x_len = 2 * w2v_c + 1\n",
        "\n",
        "    for i in range(0, len(data) - w2v_x_len):\n",
        "        Xs.append(data[i + w2v_c])\n",
        "        ts.append(data[i : i + w2v_x_len])\n",
        "\n",
        "    return Xs, ts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIr2z6imt5uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get training data\n",
        "data_Xs = []\n",
        "data_ts = []\n",
        "\n",
        "W2V_C = 3  # word2vec window size satisfying C >= 1\n",
        "W2V_X_LEN = 2 * W2V_C + 1  # training label length\n",
        "\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    data = read_data(fpath, corpus)\n",
        "    _data_Xs, _data_ts = get_data(data, w2v_c=W2V_C)\n",
        "    data_Xs += _data_Xs\n",
        "    data_ts += _data_ts\n",
        "\n",
        "data_Xs = np.array(data_Xs)\n",
        "data_ts = np.array(data_ts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J31iKGupt5ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30fe6e10-b6ee-406c-8f4b-7dd8e16c4ba3"
      },
      "source": [
        "# show sample\n",
        "data_Xs[:3]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 566,   58, 2252])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udabZUt-t5Y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6458bb48-dab6-40d4-f2bd-88fcffbd42aa"
      },
      "source": [
        "# data shape\n",
        "data_Xs.shape, data_ts.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16076, 7), (16076,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfOXJN4V3NXs",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HkycOa-esty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_dim, dim=512):\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        self.embed = torch.nn.Linear(input_dim, dim)\n",
        "        self.outs = []\n",
        "        for _ in range(C * 2):\n",
        "            self.outs.append(torch.nn.Linear(dim, input_dim))\n",
        "        self.out = torch.nn.Linear(dim, input_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embed = self.embed(input)\n",
        "\n",
        "        xs = []\n",
        "        for i in range(C * 2):\n",
        "            x = self.outs[i](embed)\n",
        "            x = F.softmax(x, dim=1)\n",
        "            xs.append(x)\n",
        "        #x = self.out(embed)\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return xs\n",
        "\n",
        "    def get_vec(self, input):\n",
        "        return self.embed(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sCXXS7Hn2L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpObq8UWrVmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "809be8c0-5167-43ca-ee0e-0d0a82e57442"
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_2uB0hmKG5Q",
        "colab_type": "text"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBQDjC5fKIpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MInibatch_Generator():\n",
        "    def __init__(self, data_size, batch_size, shuffle=True):\n",
        "        self.data_size = data_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.mbi = 0 # index for iteration\n",
        "        self.inds = np.arange(data_size)\n",
        "        np.random.shuffle(self.inds)\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.mbi + self.batch_size > self.data_size:\n",
        "            inds = self.inds[self.mbi:]\n",
        "            if self.shuffle:\n",
        "                np.random.shuffle(self.inds)\n",
        "            inds = np.hstack((inds, self.inds[ : (self.batch_size - (self.data_size - self.mbi))]))\n",
        "            self.mbi = self.batch_size - (self.data_size - self.mbi)\n",
        "        else:\n",
        "            inds = self.inds[self.mbi : self.mbi + self.batch_size]\n",
        "            self.mbi += self.batch_size\n",
        "        return inds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBu_fWQ7rZXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train\n",
        "def train():\n",
        "    # model\n",
        "    model = Word2Vec(voca_num, dim=hidden_dim).to(device)\n",
        "\n",
        "    # minibatch index\n",
        "    mb_gen = Minibatch_Generator(len(data_ts), batch_size=32)\n",
        "    mb_gen.train()\n",
        "\n",
        "    # loss function\n",
        "    loss_fn = torch.nn.NLLLoss()\n",
        "    \n",
        "    # each learning rate and iteration\n",
        "\n",
        "    # optimizer\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        \n",
        "    # each iteration\n",
        "    for ite in range(ite):\n",
        "        # get minibatch\n",
        "        mb_inds = mb_gen()\n",
        "        Xs = data_Xs[mb_inds]\n",
        "        ts = data_ts[mb_inds]\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ts = model(Xs)\n",
        "\n",
        "        ts = \n",
        "\n",
        "        loss = 0.\n",
        "        accuracy = 0.\n",
        "        total_len = 0\n",
        "\n",
        "        # each data of minibatch\n",
        "        for mb_index in range(mb):\n",
        "            # 1 data of minibatch\n",
        "            Xs = np.array(X_inds[mb_index]).reshape([-1, 1])\n",
        "\n",
        "            input_X = np.zeros([1, voca_num])\n",
        "            input_X[:, Xs[C]] = 1\n",
        "            input_X = torch.tensor(input_X, dtype=torch.float).to(device)\n",
        "            \n",
        "            # reset graph\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "            # data length\n",
        "            total_len += x_length\n",
        "\n",
        "            # forward network\n",
        "            ys = model(input_X)\n",
        "\n",
        "            # target label index\n",
        "            t_inds = [_i for _i in range(x_length) if _i != C]\n",
        "\n",
        "            # each target label\n",
        "            for i, y in zip(t_inds, ys):\n",
        "                # target label\n",
        "                t = torch.tensor(Xs[i], dtype=torch.long).to(device)\n",
        "\n",
        "                # get loss\n",
        "                loss += loss_fn(torch.log(y), t)\n",
        "\n",
        "                # count accuracy\n",
        "                if y.argmax() == t:\n",
        "                    accuracy += 1\n",
        "\n",
        "            \"\"\"\n",
        "            # each target label\n",
        "            for i in range(x_length):\n",
        "                # forward network\n",
        "                y = model(input_X)\n",
        "\n",
        "                # target label\n",
        "                t = torch.tensor(Xs[i], dtype=torch.long).to(device)\n",
        "                #t = torch.tensor(Xs[i], dtype=torch.long).to(device).view(-1, voca_num)\n",
        "\n",
        "                # get loss\n",
        "                loss += loss_fn(torch.log(y), t)\n",
        "\n",
        "                # count accuracy\n",
        "                if y.argmax() == t:\n",
        "                    accuracy += 1\n",
        "            \"\"\"\n",
        "\n",
        "        # loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # update weight\n",
        "        optimizer.step()\n",
        "        \n",
        "        # get loss\n",
        "        loss = loss.item() / total_len\n",
        "        accuracy = accuracy / total_len\n",
        "\n",
        "        if (ite + 1) % 10 == 0:\n",
        "            print(\"iter :\", ite+1, \",loss >>:\", loss, \"accuracy:\", accuracy)\n",
        "\n",
        "    torch.save(model.state_dict(), 'word2vec.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}