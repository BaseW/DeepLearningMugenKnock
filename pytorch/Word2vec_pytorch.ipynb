{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WJM5vNrDyIHSMIyxLVNbPvuYSI91jaZX",
      "authorship_tag": "ABX9TyNWY5UrMQYD4VgOMA76w4P1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoyoyo-yo/DeepLearningMugenKnock/blob/master/pytorch/Word2vec_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOcYMnxbsa_l",
        "colab_type": "text"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHNQSDFxhp_",
        "colab_type": "text"
      },
      "source": [
        "# import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8ywkt2AxjCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG65jE34sX7Y",
        "colab_type": "text"
      },
      "source": [
        "# Ginza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5b0-R8Ase_Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "d61f0b26-ca69-47c7-8eb7-7e2540df0e9c"
      },
      "source": [
        "!pip install ginza"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ginza\n",
            "  Downloading https://files.pythonhosted.org/packages/93/43/43818861210c71a0a9f789e7350b785ba60bad38196745c2f8e88271dfa8/ginza-3.1.2.tar.gz\n",
            "Requirement already satisfied: spacy>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from ginza) (2.2.4)\n",
            "Collecting ja_ginza<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/02/ad08f43df50c5d877a7b2dcd56f4ebf33a5818b517550516e8ba059069fa/ja_ginza-3.1.0.tar.gz (54.9MB)\n",
            "\u001b[K     |████████████████████████████████| 54.9MB 56kB/s \n",
            "\u001b[?25hCollecting SudachiPy>=0.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/f9/297cfd6e16031ef71c9d9f23c224908f70f311e32786475aa4f8e82e4775/SudachiPy-0.4.8.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (47.1.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.3->ginza) (7.4.0)\n",
            "Collecting ja_ginza_dict<3.2.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/e5/c4ea3f165acb38fa125e992d1a6fcfb9a5a8ab6a14cc5010c836713c386b/ja_ginza_dict-3.1.0-1.tar.gz (44.8MB)\n",
            "\u001b[K     |████████████████████████████████| 44.8MB 66kB/s \n",
            "\u001b[?25hCollecting sortedcontainers~=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/13/f3/cf85f7c3a2dbd1a515d51e1f1676d971abe41bba6f4ab5443240d9a78e5b/sortedcontainers-2.1.0-py2.py3-none-any.whl\n",
            "Collecting dartsclone~=0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/34/987a076369ed086ee953e2f0b9ab5ff3e1a682ba4f781678ac5648144896/dartsclone-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (474kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.3->ginza) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.3->ginza) (3.0.4)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from dartsclone~=0.9.0->SudachiPy>=0.4.2->ginza) (0.29.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.3->ginza) (3.1.0)\n",
            "Building wheels for collected packages: ginza, ja-ginza, SudachiPy, ja-ginza-dict\n",
            "  Building wheel for ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ginza: filename=ginza-3.1.2-cp36-none-any.whl size=17311 sha256=6438676eff8fc9e514c0dd6e2ce9fb717766065b25b5574e8f32f5751a88e4c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/8d/57/f089078acc0dbaebffc08c178e9f20924fa794c114ad36f7f7\n",
            "  Building wheel for ja-ginza (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ja-ginza: filename=ja_ginza-3.1.0-cp36-none-any.whl size=54963619 sha256=461f5a5f07d477212ecae03435955364ea78caee859ddf6663042e43d44f90b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/8a/07/1837eeb5c5648fa8d266102b78a894e495234585ac3f024cf1\n",
            "  Building wheel for SudachiPy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for SudachiPy: filename=SudachiPy-0.4.8-cp36-cp36m-linux_x86_64.whl size=870871 sha256=8c26b1aed0d152ac71bc9f28aeaf8fb786ed88377b1791d87f8bc41972e9db5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/01/f5/7fd64fc5305acb528a4f8a04c8b2c0641b9ceb05e0998e4bc4\n",
            "  Building wheel for ja-ginza-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ja-ginza-dict: filename=ja_ginza_dict-3.1.0-cp36-none-any.whl size=70877544 sha256=87cb7d27b6b48b1dcce4dbec15ccdf70e21b9ef88eda21a5bf78908d2cc6a6cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/88/d7/7f0692ba26060966af34538e1079438d16640c54e04a15a76a\n",
            "Successfully built ginza ja-ginza SudachiPy ja-ginza-dict\n",
            "Installing collected packages: ja-ginza-dict, ja-ginza, sortedcontainers, dartsclone, SudachiPy, ginza\n",
            "  Found existing installation: sortedcontainers 2.2.2\n",
            "    Uninstalling sortedcontainers-2.2.2:\n",
            "      Successfully uninstalled sortedcontainers-2.2.2\n",
            "Successfully installed SudachiPy-0.4.8 dartsclone-0.9.0 ginza-3.1.2 ja-ginza-3.1.0 ja-ginza-dict-3.1.0 sortedcontainers-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXHHMNq7tN6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pkg_resources, imp\n",
        "imp.reload(pkg_resources)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('ja_ginza')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9G9Xg7Ms_Zu",
        "colab_type": "text"
      },
      "source": [
        "# Get corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV6jrnwKshi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus(fname):\n",
        "    corpus = []\n",
        "\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _corpus = []\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    _corpus.append(token.orth_)\n",
        "\n",
        "            corpus = list(set(corpus) | set(_corpus))\n",
        "    corpus.sort()\n",
        "    return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxNN4BkHshgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample\n",
        "corpus = get_corpus('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt')\n",
        "corpus = ['<UNKNOWN>'] + corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFPK7BsVt7p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    _corpus = get_corpus(fpath)\n",
        "    corpus = list(set(corpus) | set(_corpus))\n",
        "\n",
        "corpus.sort()\n",
        "\n",
        "corpus = ['<UNKNOWN>'] + corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omOOGN9y1TJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4cd9d35f-a669-4596-885c-16c891530430"
      },
      "source": [
        "# show sample\n",
        "corpus[:20]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<UNKNOWN>',\n",
              " '!',\n",
              " '(',\n",
              " ')',\n",
              " '.co.jp',\n",
              " '1',\n",
              " '10',\n",
              " '100',\n",
              " '10万',\n",
              " '110',\n",
              " '12',\n",
              " '146',\n",
              " '2',\n",
              " '20',\n",
              " '200',\n",
              " '2007',\n",
              " '2008',\n",
              " '2009',\n",
              " '2010',\n",
              " '211']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJR93YOD1mjd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50af104c-b83f-446c-f5e7-3e05606758aa"
      },
      "source": [
        "# corpus len\n",
        "len(corpus)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2151"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5MAviutx16",
        "colab_type": "text"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A41KZI6dtTYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(fname, corpus):\n",
        "    Xs = []\n",
        "    with open(fname, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip()\n",
        "            _Xs = [corpus.index('<SOS>')]\n",
        "            for sent in nlp(line).sents:\n",
        "                for token in sent:\n",
        "                    w = token.orth_\n",
        "\n",
        "                    if w in corpus:\n",
        "                        ind = corpus.index(w)\n",
        "                    else:\n",
        "                        ind = corpus.index('<UNKNOWN>')\n",
        "                    _Xs.append(ind)\n",
        "            _Xs.append(corpus.index('<EOS>'))\n",
        "            Xs.append(_Xs)\n",
        "\n",
        "    return np.array(Xs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlqoowV0Bpr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(fname, corpus):\n",
        "    Xs = []\n",
        "    with open(fname, 'r') as f:\n",
        "        data = f.read().rstrip()\n",
        "\n",
        "    Xs = [corpus.index(token.orth_) if token.orth_ in corpus else corpus.index('<UNKNOWN>') for sent in nlp(data).sents for token in sent]\n",
        "\n",
        "    return np.array(Xs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yztiXC6Y0sVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read_data('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/sandwitchman_hanayome_original.txt', corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRprBc1sshda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data, w2v_c=None):\n",
        "    Xs = []\n",
        "    ts = []\n",
        "\n",
        "    w2v_x_len = 2 * w2v_c + 1\n",
        "\n",
        "    for i in range(0, len(data) - w2v_x_len):\n",
        "        Xs.append(data[i + w2v_c])\n",
        "        ts.append(data[i : i + w2v_x_len])\n",
        "\n",
        "    return Xs, ts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIr2z6imt5uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get training data\n",
        "data_Xs = []\n",
        "data_ts = []\n",
        "\n",
        "W2V_C = 3  # word2vec window size satisfying C >= 1\n",
        "W2V_X_LEN = 2 * W2V_C + 1  # training label length\n",
        "\n",
        "for fpath in glob('/content/drive/My Drive/Colab Notebooks/datasets/sandwitchman/*_original.txt'):\n",
        "    data = read_data(fpath, corpus)\n",
        "    _data_Xs, _data_ts = get_data(data, w2v_c=W2V_C)\n",
        "    data_Xs += _data_Xs\n",
        "    data_ts += _data_ts\n",
        "\n",
        "data_Xs = np.array(data_Xs)\n",
        "data_ts = np.array(data_ts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J31iKGupt5ld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d3ddbf7-8644-4dd3-b303-d2ea5af24248"
      },
      "source": [
        "# show sample\n",
        "data_Xs[:3]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 454,   58, 2067])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udabZUt-t5Y3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a006dabb-64d6-4be8-a606-0a115217d794"
      },
      "source": [
        "# data shape\n",
        "data_Xs.shape, data_ts.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16513,), (16513, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9sJYLFHcbR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfOXJN4V3NXs",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HkycOa-esty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec(torch.nn.Module):\n",
        "    def __init__(self, input_dim, dim=512, w2v_c=2):\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        self.w2v_c = w2v_c\n",
        "\n",
        "        #self.embed = torch.nn.Linear(input_dim, dim)\n",
        "        self.embed = torch.nn.Sequential(\n",
        "            torch.nn.Embedding(input_dim, dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(dim, dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.outs = []\n",
        "        for _ in range(w2v_c * 2):\n",
        "            self.outs.append(torch.nn.Linear(dim, input_dim))\n",
        "        self.outs = torch.nn.ModuleList(self.outs)\n",
        "        #self.out = torch.nn.Linear(dim, input_dim)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embed = self.embed(input)\n",
        "\n",
        "        xs = []\n",
        "        for i in range(self.w2v_c * 2):\n",
        "            x = self.outs[i](embed)\n",
        "            #x = F.softmax(x, dim=1)\n",
        "            xs.append(x)\n",
        "        #x = self.out(embed)\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return xs\n",
        "\n",
        "    def get_vec(self, input):\n",
        "        return self.embed(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sCXXS7Hn2L7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpObq8UWrVmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cf3fc6a-e29c-4eba-e7fb-cb55e2fa86c1"
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_2uB0hmKG5Q",
        "colab_type": "text"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBQDjC5fKIpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Minibatch_Generator():\n",
        "    def __init__(self, data_size, batch_size, shuffle=True):\n",
        "        self.data_size = data_size\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.mbi = 0 # index for iteration\n",
        "        self.inds = np.arange(data_size)\n",
        "        np.random.shuffle(self.inds)\n",
        "\n",
        "    def __call__(self):\n",
        "        if self.mbi + self.batch_size > self.data_size:\n",
        "            inds = self.inds[self.mbi:]\n",
        "            if self.shuffle:\n",
        "                np.random.shuffle(self.inds)\n",
        "            inds = np.hstack((inds, self.inds[ : (self.batch_size - (self.data_size - self.mbi))]))\n",
        "            self.mbi = self.batch_size - (self.data_size - self.mbi)\n",
        "        else:\n",
        "            inds = self.inds[self.mbi : self.mbi + self.batch_size]\n",
        "            self.mbi += self.batch_size\n",
        "        return inds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHopXNeva0l3",
        "colab_type": "text"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHzBjIxAdAII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensor\n",
        "data_Xs = torch.tensor(data_Xs, dtype=torch.long).to(DEVICE)\n",
        "data_ts = torch.tensor(data_ts, dtype=torch.long).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBu_fWQ7rZXP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "beed1e84-7202-4477-cbfe-cfcdc7fc6cbc"
      },
      "source": [
        "# train\n",
        "def train():\n",
        "    # model\n",
        "    model = Word2Vec(len(corpus), dim=128, w2v_c=W2V_C).to(DEVICE)\n",
        "    model.train()\n",
        "\n",
        "    # minibatch index\n",
        "    Minibatch_size = 64\n",
        "    mb_gen = Minibatch_Generator(len(corpus), batch_size=Minibatch_size)\n",
        "\n",
        "    # loss function\n",
        "    #loss_fn = torch.nn.NLLLoss()\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    # each learning rate and iteration\n",
        "\n",
        "    # optimizer\n",
        "    opt = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    \n",
        "        \n",
        "    # each iteration\n",
        "    for ite in range(100_000):\n",
        "        # get minibatch\n",
        "        mb_inds = mb_gen()\n",
        "        Xs = data_Xs[mb_inds]\n",
        "        ts = data_ts[mb_inds]\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ys = model(Xs) # feed forward\n",
        "\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "\n",
        "        for i in range(W2V_C):\n",
        "            loss += loss_func(ys[i], ts[:, i])\n",
        "            preds = ys[i].argmax(dim=1, keepdim=True).reshape(-1)\n",
        "            accuracy += (preds == ts[:, i]).sum() / float(Minibatch_size)\n",
        "\n",
        "            loss += loss_func(ys[W2V_X_LEN - i - 2], ts[:, W2V_X_LEN - i - 1])\n",
        "            preds = ys[W2V_X_LEN - i - 2].argmax(dim=1, keepdim=True).reshape(-1)\n",
        "            accuracy += (preds == ts[:, W2V_X_LEN - i - 1]).sum() / float(Minibatch_size)\n",
        "\n",
        "        # loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # update weight\n",
        "        opt.step()\n",
        "        \n",
        "        # get loss\n",
        "        loss = loss.item() / W2V_X_LEN\n",
        "        accuracy = accuracy.item() / W2V_X_LEN\n",
        "\n",
        "        print('\\riter:{} , L:{:.4f}, A:{:.4f}'.format(ite+1, loss, accuracy), end='')\n",
        "\n",
        "        if (ite + 1) % 1000 == 0:\n",
        "            print()\n",
        "\n",
        "    torch.save(model.state_dict(), 'word2vec.pt')\n",
        "\n",
        "train()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter:1000 , L:4.1437, A:0.1094\n",
            "iter:2000 , L:3.9257, A:0.1049\n",
            "iter:3000 , L:3.4315, A:0.1741\n",
            "iter:4000 , L:3.1370, A:0.1987\n",
            "iter:5000 , L:3.0680, A:0.1652\n",
            "iter:6000 , L:2.5537, A:0.1942\n",
            "iter:7000 , L:2.5538, A:0.1786\n",
            "iter:8000 , L:2.1660, A:0.2321\n",
            "iter:9000 , L:2.1832, A:0.2478\n",
            "iter:10000 , L:2.2630, A:0.2679\n",
            "iter:11000 , L:2.2149, A:0.2254\n",
            "iter:12000 , L:1.8999, A:0.2812\n",
            "iter:13000 , L:1.7796, A:0.2991\n",
            "iter:14000 , L:2.0436, A:0.2924\n",
            "iter:15000 , L:2.0649, A:0.2321\n",
            "iter:16000 , L:1.6473, A:0.3750\n",
            "iter:17000 , L:1.9071, A:0.2835\n",
            "iter:18000 , L:2.0826, A:0.2612\n",
            "iter:19000 , L:1.9977, A:0.2701\n",
            "iter:20000 , L:1.8735, A:0.3013\n",
            "iter:21000 , L:1.9757, A:0.2679\n",
            "iter:22000 , L:2.0714, A:0.2500\n",
            "iter:23000 , L:1.6632, A:0.3125\n",
            "iter:24000 , L:1.4600, A:0.3973\n",
            "iter:25000 , L:2.0889, A:0.2366\n",
            "iter:26000 , L:1.7314, A:0.3080\n",
            "iter:27000 , L:1.9004, A:0.2946\n",
            "iter:28000 , L:2.0216, A:0.2567\n",
            "iter:29000 , L:1.7888, A:0.2589\n",
            "iter:30000 , L:1.5828, A:0.3393\n",
            "iter:31000 , L:1.9508, A:0.2612\n",
            "iter:32000 , L:1.8320, A:0.3125\n",
            "iter:33000 , L:2.1318, A:0.1897\n",
            "iter:34000 , L:1.7017, A:0.3259\n",
            "iter:35000 , L:1.7218, A:0.3683\n",
            "iter:36000 , L:1.7512, A:0.2679\n",
            "iter:37000 , L:2.0019, A:0.2612\n",
            "iter:38000 , L:1.8494, A:0.2924\n",
            "iter:39000 , L:1.5797, A:0.3616\n",
            "iter:40000 , L:1.7224, A:0.3326\n",
            "iter:41000 , L:2.0630, A:0.2344\n",
            "iter:42000 , L:1.6986, A:0.3103\n",
            "iter:43000 , L:1.8224, A:0.3036\n",
            "iter:44000 , L:1.9515, A:0.2879\n",
            "iter:45000 , L:1.8195, A:0.2388\n",
            "iter:46000 , L:2.0108, A:0.2277\n",
            "iter:47000 , L:2.0406, A:0.2388\n",
            "iter:48000 , L:1.5399, A:0.4085\n",
            "iter:49000 , L:1.9743, A:0.2232\n",
            "iter:50000 , L:1.7222, A:0.2768\n",
            "iter:51000 , L:1.6363, A:0.3438\n",
            "iter:52000 , L:1.8617, A:0.3058\n",
            "iter:53000 , L:1.8314, A:0.2701\n",
            "iter:54000 , L:1.8079, A:0.2589\n",
            "iter:55000 , L:1.9854, A:0.2321\n",
            "iter:56000 , L:2.0349, A:0.2812\n",
            "iter:57000 , L:1.7367, A:0.2835\n",
            "iter:58000 , L:1.5983, A:0.3304\n",
            "iter:59000 , L:1.8471, A:0.2835\n",
            "iter:60000 , L:1.6881, A:0.3348\n",
            "iter:61000 , L:1.8766, A:0.2076\n",
            "iter:62000 , L:1.9703, A:0.2455\n",
            "iter:63000 , L:1.8825, A:0.3058\n",
            "iter:64000 , L:1.9267, A:0.2612\n",
            "iter:65000 , L:1.9739, A:0.2277\n",
            "iter:66000 , L:2.0373, A:0.2522\n",
            "iter:67000 , L:1.9234, A:0.2522\n",
            "iter:68000 , L:1.7961, A:0.3036\n",
            "iter:69000 , L:2.0290, A:0.2522\n",
            "iter:70000 , L:1.7333, A:0.2768\n",
            "iter:71000 , L:1.6604, A:0.3326\n",
            "iter:72000 , L:2.1112, A:0.2589\n",
            "iter:73000 , L:1.9459, A:0.2924\n",
            "iter:74000 , L:1.9393, A:0.2746\n",
            "iter:75000 , L:1.9108, A:0.2879\n",
            "iter:76000 , L:2.0048, A:0.2790\n",
            "iter:77000 , L:2.0079, A:0.2254\n",
            "iter:78000 , L:1.7668, A:0.3147\n",
            "iter:79000 , L:1.7333, A:0.3125\n",
            "iter:80000 , L:1.9692, A:0.3013\n",
            "iter:81000 , L:1.8059, A:0.3147\n",
            "iter:82000 , L:1.6752, A:0.2946\n",
            "iter:83000 , L:1.9400, A:0.2835\n",
            "iter:84000 , L:2.0040, A:0.2500\n",
            "iter:85000 , L:1.8160, A:0.2969\n",
            "iter:86000 , L:1.9862, A:0.2701\n",
            "iter:87000 , L:2.0308, A:0.2522\n",
            "iter:88000 , L:2.0895, A:0.2478\n",
            "iter:89000 , L:1.6578, A:0.3527\n",
            "iter:90000 , L:1.9435, A:0.2924\n",
            "iter:91000 , L:2.0287, A:0.2232\n",
            "iter:92000 , L:1.8533, A:0.3326\n",
            "iter:93000 , L:1.8100, A:0.3304\n",
            "iter:94000 , L:1.7452, A:0.3103\n",
            "iter:95000 , L:1.8921, A:0.2857\n",
            "iter:96000 , L:1.6120, A:0.3214\n",
            "iter:97000 , L:1.8453, A:0.2835\n",
            "iter:98000 , L:1.7896, A:0.2902\n",
            "iter:99000 , L:1.9394, A:0.2589\n",
            "iter:100000 , L:1.7061, A:0.3504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncePZaEsaZ_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK1FmuIIkf1W",
        "colab_type": "text"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Inl-yLSDki1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "88a38a4a-6a83-4447-e514-ead59c0edc78"
      },
      "source": [
        "corpus[-30 : ]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['麻生',\n",
              " '黒',\n",
              " '黒い',\n",
              " '黙秘権',\n",
              " '黙読',\n",
              " '鼻',\n",
              " '鼻毛',\n",
              " '鼻血',\n",
              " '！',\n",
              " '（',\n",
              " '）',\n",
              " '－',\n",
              " '０',\n",
              " '１',\n",
              " '１日',\n",
              " '１番',\n",
              " '２',\n",
              " '２人',\n",
              " '３',\n",
              " '４',\n",
              " '５',\n",
              " '５千',\n",
              " '７',\n",
              " '？',\n",
              " 'ＤＶ',\n",
              " 'ＫＡＮ',\n",
              " 'ＯＫ',\n",
              " 'ＵＶ',\n",
              " 'Ｖ',\n",
              " '～']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t8d2lj4kgmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = '鼻毛'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fexh1lxPkpqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus_vecs(model):\n",
        "    batch_size = 1024\n",
        "    corpus_vecs = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(corpus), batch_size):\n",
        "            Xs = np.arange(i , min(i + batch_size, len(corpus)))\n",
        "            Xs = torch.tensor(Xs, dtype=torch.long).to(DEVICE)\n",
        "            _vecs = model.get_vec(Xs)\n",
        "            _vecs = _vecs.detach().cpu().numpy()\n",
        "            corpus_vecs = np.vstack([corpus_vecs, _vecs]) if corpus_vecs is not None else _vecs\n",
        "\n",
        "    return corpus_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOlrvU8uksPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_vec(model, word):\n",
        "    word_i = corpus.index(word)\n",
        "\n",
        "    Xs = torch.tensor(word_i, dtype=torch.long).to(DEVICE).reshape([1, -1])\n",
        "    with torch.no_grad():\n",
        "        vec = model.get_vec(Xs)\n",
        "        vec = vec.detach().cpu().numpy()\n",
        "    return vec[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Kgg7AhnkRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_near_word(word):\n",
        "    model = Word2Vec(len(corpus), dim=128, w2v_c=W2V_C).to(DEVICE)\n",
        "    model.load_state_dict(torch.load('word2vec.pt', map_location=torch.device(DEVICE)))\n",
        "    model.eval()\n",
        "\n",
        "    corpus_vecs = get_corpus_vecs(model) # corpus > vec\n",
        "\n",
        "    word_vec = get_word_vec(model, word) # word > vec\n",
        "    \n",
        "    # cosine similarity\n",
        "    #scores = np.abs(corpus_vecs - word_vec).sum(axis=1)\n",
        "    norm_A = np.linalg.norm(corpus_vecs, axis=1)\n",
        "    norm_B = np.linalg.norm(word_vec)\n",
        "    scores = np.dot(corpus_vecs, word_vec.T)[:, 0] / norm_A / norm_B\n",
        "\n",
        "    # get near word\n",
        "    arg_inds = scores.argsort()[::-1]\n",
        "    near_word = np.array(corpus)[arg_inds]\n",
        "    near_scores = scores[arg_inds]\n",
        "    return near_word, near_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmeQ9-NFoQhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_rank, word_scores = get_near_word(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "157xyI5XoSja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3db9c1d2-7816-4916-da1d-8bb91ca3d25e"
      },
      "source": [
        "word_rank"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['鼻毛', 'ＯＫ', '淳二', ..., 'でしょ', 'でし', 'A'], dtype='<U15')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8jJhYdoNMXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "fdcac830-8166-430f-8adb-48917ab21df9"
      },
      "source": [
        "pip install japanize-matplotlib # 日本語matplotlob"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting japanize-matplotlib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/aa/3b24d54bd02e25d63c8f23bb316694e1aad7ffdc07ba296e7c9be2f6837d/japanize-matplotlib-1.1.2.tar.gz (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 94kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from japanize-matplotlib) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->japanize-matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->japanize-matplotlib) (1.12.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.2-cp36-none-any.whl size=4120191 sha256=594435b3ea9236c3c5002830ae3065e15e0cea8de39cd9136e728c15b331ef3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/f9/fc/bc052ce743a03f94ccc7fda73d1d389ce98216c6ffaaf65afc\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWWHjCuNQCU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "97ea8272-3393-449e-b04a-962140dd1d98"
      },
      "source": [
        "import japanize_matplotlib #日本語化matplotlib\n",
        "import seaborn as sns\n",
        "sns.set(font=\"IPAexGothic\") #日本語フォント設定"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/japanize_matplotlib/japanize_matplotlib.py:15: MatplotlibDeprecationWarning: \n",
            "The createFontList function was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use FontManager.addfont instead.\n",
            "  font_list = font_manager.createFontList(font_files)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoQGQmj_pNAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "cea3a90e-7cf5-41c1-a81a-95492a605198"
      },
      "source": [
        "# display top N\n",
        "top_N = 10\n",
        "xs = np.arange(top_N, 0, -1)\n",
        "plt.barh(xs, word_scores[:top_N])\n",
        "plt.yticks(xs, word_rank[:top_N])\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD8CAYAAABdCyJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de1RU9doH8O9wE1FQIdEEkUIU0KyE9S4LKfLyhoVoQ3FROQc9YqYWYVlm6iFT83pSVKzTAS0DVCpN0dRMj1eOV0wT8UKBDCnITW4KzMx+//BtThOSo3tmNsP+ftY6q8WePdvn8aTf9m9m/x6FIAgCiIiIHpCV1AUQEZFlY5AQEZEoDBIiIhKFQUJERKIwSIiISBQGCRERicIgISIiUWykLkAqlZV10Grl9wiNi0tHlJfXSl2GZNg/+2f/D9a/lZUCXbp0uOtrsg0SrVaQZZAAkG3fv2H/7F/OTNE/l7aIiEgUBgkREYnCICEiIlEYJEREJAqDhIiIRGGQEBGRKAwSIiISRcHBVkRE8tDYpMHNqvoHeq+VlQIuLh3v+ppsH0j82/w9KK28JXUZRERms335KJNcl0tbREQkSqsJkuzsbOzcuVPqMoiI6D5JvrQVGxsLtVoNhUIBAMjIyEBDQwO8vb2xYMGCFt8nCALOnz8PtVqNJ554wlzlEhHRH0geJIa4efMmioqKcPXqVVy5cgV5eXmoqamBp6cngoKCpC6PiEjWWkWQjBgxAlZW/11lq6qqgkql0v184MAB5Ofno2fPnigrK8OQIUPw8ssvS1EqERH9geRBYm9vj127dukda2pqgp+fn+7nkJAQVFRUoKysDD/++CNOnjyJiooK3f+CgoIwcuRIc5dORESQOEh27twJf39/tG/fHgBQUFAAT09PAEB1dTUOHz6MwYMHY/369SgqKoKzszNu3LiBhx9+GP3794eLiwtcXFzQpUsXCbsgIrIcXbs6Gv2akgbJuXPncOzYMXTocGfqVl5eHnx8fAAAlZWVcHBwwODBgzFp0iTde1JSUmBvbw83NzeUl5ejqKgIFRUVGDZsGJydnSXpg4jIUty4UfNA72u1DyR6enrC1dUVVVVV6NKlC7p37w5fX1/U1NTA1tYWPXr0wK+//oqFCxeisbERCoUCRUVFcHR0xM8//wxnZ2e4uLjgoYcegp2dnZStEBHJlmRB8s033yArKwsAcPXqVXTu3BlOTk7Iy8tDXV0d3NzckJ2djdjYWPzjH//QBUVKSgq6dOkCpVKpu9bmzZtRU1ODjh3vnpZERGQ6kgWJUqmEUqnE9evXERcXh02bNsHe3h5Hjx7FsWPHkJCQcNf3KRQKVFVVQavVArjz1eCvv/4a/v7+5iyfiIj+n+Qftq9duxaJiYmwt7c36D2BgYFYuHAhvv32W2i1WrRv3x4DBgyAl5eXiaslIqK7kXT331u3bsHW1hY2NubPM27aSERys335qLb3YftvX/slIiLLxXkkREQywXkkRlZeXgutVn4Z2rWr4wPf2rYF7J/9y71/U2g128gTEZFlYpAQEZEosl3aammtTw5MdXtrKVpb/7cb1Kip5jcIyXLJNkj49V9qLbYvHwX5rtpTW8ClLSIiEoVBQkREojBIiIhIlFYRJCdPnkR4eDiee+45jBw5Evv27ZO6JCIiMpDkQZKfn4+3334bf//737F//36sWbMGixcvRnZ2tt55q1atQkpKiu7nEydOYNiwYcjLyzN3yURE9DuSB0lycjLi4uIwYMAAAICHhwdmzJiBlStXtvie7OxszJ49G//85z91ExWJiEgakn/999ixY3jrrbf0jj377LOIj4/HrVu3mm3sePjwYSxatAgpKSlwd3c3Z6lEJmPOZ1ta23M05sb+29jMdkEQUF5ejoceekjvuK2tLRwcHFBTU6MXJEePHkVmZiYcHBzg4uJi7nKJTMZc+z9xryn2b4pt5CVd2lIoFOjevTtKS0v1jtfV1eHWrVvNwqKyshIbN27E//zP/2DWrFnmLJWIiFog+Wckzz77LL777ju9Yzt37kRQUBCsra31jr/44ovo3Lkz3n77bVy/fh2ffPKJOUslIqK7kDxIpkyZgo0bN2Lv3r1oaGhAdnY21qxZgzfffLPF99jY2GDFihX48ssv+VVhIiKJSR4krq6uSE1NxXfffYeIiAhkZGRg7dq16Nu3r955np6e8PDw0P3crVs3LFu2DDt27EBdXZ25yyYiov8n+be2AKBXr15Yvnz5n54zcuTIZscGDRqEQYMGmaosIiIygOR3JEREZNlaxR2JFFJm/6/UJRABuDOPhMiSyTZIOLNdnuTeP5EpcGmLiIhEYZAQEZEosl3a4sx2+Wot/XNWO7UVsg0SzmwnqXFWO7UVXNoiIiJRGCRERCSKpEGyceNG1NfX6x1LSEhAYWGhRBUREdH9kvQzkocffhiTJk1CREQEzpw5AwCwsrJCamqqbuffiIgI3Lx5E3//+99bvM78+fMREBBglpqJiEifpEHy2GOPYe3atXB0dMT+/ftRVlYGALr5JLGxsbpRurt27ZKsTiIiaplkQVJXV4e//vWveOeddxAUFITi4mLMnz9f9/qhQ4eaDbwiIqLWR7Ig6dChA1JTU5GamoqgoCBERETg6NGjutetra0xcOBAqcojMgspnmlpLc/RSIX9t7GZ7V27dsXrr78OlUqFrKws3fGmpiZcu3YNnTp1ajaXhKgtMfe+X3Lfa4z9m2Zmu6RBotFoMHHiRKSlpWH9+vUAgHXr1qG4uBjz5s2Dl5eXlOUREZEBJA2S7du348knn8TChQuRl5cHACgoKEDXrl1x8eJFAMArr7yCVatW3fNa6enp6Nq1q0nrJSKi5iQLkurqaqxZswZffvklunXrpjv+wQcfICIiAr6+vrpjYWFhUpRIREQGkOyBxJKSEowdO1YXIoWFhVAqlfj3v/8NZ2dnqcoiIqL7JNkdibe3N7y9vXU/9+rVC998841U5RAR0QPiXltERCSKbLeR58x2khpntVNbIdsg4cx2eZJ7/0SmwKUtIiIShUFCRESiyHZpizPb5Uvq/jmrndoa2QYJZ7aTVDirndoaLm0REZEoDBIiIhLF6EtbZ86cwfHjx+Ht7Y3r168jOjoamZmZcHd3R3FxMbp164agoCDd+YGBgThy5IixyyAiIjMxapCcOHECGzZsQGlpKU6dOoX6+nrY2Njg+PHjGD58OL7++mvMnTtXd74gCFAoFPe87rVr12BjY8PdfYmIWiGjLm0FBARg+PDhGDJkCIYNG4ahQ4di2LBhOHbsGF5//XUcOnQIU6dOxffffw/gzjwSa2vre15369at2LNnj94xrVaLxYsXo6GhwZgtEBHRfTJqkGg0GmRmZqJ9+/a4cOECVCoVPv30U5SWlmLDhg3w9/fHhg0bMHz4cAB3wsCQILG3t0dTU5N+4VZW6NChA5YvX27MFoiI6D4ZdWlLoVCgvr4eO3fuRG1tLTp37ozy8nIEBAQgNjYWFy9eRGxsLGbNmoU+ffpAo9HAyureWebk5ITi4uJmxydPnoxXXnkFeXl58PHxMWYrRCYl5bMsUj9HIzX238pntjc2NiIwMBDFxcUoKyvD448/jsGDB2PdunVITk7GlClTkJycrDu/paWt4uJidO7cGR06dAAA9OjRAydOnGhevI0NYmJikJqaiiVLlhizFSKTkmq/L7nvNcb+TTOz3ahLW9XV1fD390efPn3g7u4Of39/ODg4AABiY2ORk5OD2NhYXLp0CQBgbW0NrVard43Gxka8++67OH36tO5Yv3798NNPPwEAioqKsG7dOt1rL7zwAm7evNnsOkREZB5GDZKGhgbU19dDpVLh/PnzqKqqQl1dHQBg/fr1ePLJJ7F+/Xr06dMHANC+fXtotVpcvXoVAKBSqfDaa6/B399f7yvCTk5OcHd3x8GDB7Fu3TrY2dnpXrO3t8enn35q0BIZEREZn1H/9vXw8EC/fv1QUFCAuLg4fPbZZ2jfvj0AICYmBqdOnUJMTAwyMzN170lMTMTUqVPx/PPPIyEhAZGRkUhISGh27Tlz5mDp0qW4du0aIiMjjVk2ERGJYNTPSFQqFeLi4rBy5Ur07dsXrq6uus9ANmzYcNf3BAUF6d19tMTNzQ3bt283ZrlERGQERg0Sd3d3bN++Hba2tgDuPFcCQO8DdiIialuM/sHCbyFCRETyINtt5DmznaTCWe3U1sg2SDizXZ7k3j+RKfA7s0REJAqDhIiIRJHt0hZntsuXVP1zVju1VbINEs5sJ3PjrHZqq7i0RUREojBIiIhIlFYRJI2NjXjrrbegVrf8/fqCggIsWLDAjFUREZEhWkWQbN68GY6OjrCxafkjmxMnTsDV1RWFhYVobGzEzZs3/zR4iIjIPCT/sL2hoQHp6en48ssv7/p6YmIifvrpJ6hUKjz88MM4f/48EhMT8f3332P37t1YtWqVbodhIiIyP8mDZOPGjXjhhRfg7Ox819cTExNRVlaGKVOmYPPmzQAAtVqNsLAwlJeXIyEhAWvWrDFo9jsRERmfpEHy293Ic889h7i4OLz//vvw9PRsdt6GDRugVCp1P//nP//B8ePHMX36dOzevZtDrchitIZneFpDDVJi/618Zvv92rJlCyorKxEUFAQfHx+kpaXh/fffB3AnZNq1a4fS0lLs2rULW7Zs0b3PwcEB9fX1AIDnn39ektqJHoTU+3zJfa8x9m+ame2SBYlWq0VqaiqWLFmCwMBAXL58GYcOHQJwJ0TGjBmDzZs3Y968eZg4cSIcHBzw/fffY+PGjVCr1bCzs0NjY6Pe2F0iIjI/ydaEdu/ejT59+iA4OBgA4OLigqtXr0IQBCxatAhDhw6FtbU1oqOjER4ejrKyMqSlpWHp0qWIiYlBXl4eQkJCsGvXLqlaICIiSBgkBw8exNSpU3U/Ozs7o3fv3nj22WdhbW2NyZMnAwACAwNhZWUFQRAgCALs7OxQUlKC0NBQJCcn48CBA1K1QEREABSCIFjMUI5du3YhMzMTTk5OePfdd9G9e/cHvhb32iJz2758lOTr8/yMgP23qc9IHkRISAhCQkKkLoOIiH6H35slIiJRGCRERCSKRS1tGVPK7P+VugSSmdsN3BuO2ibZBkl5eS20Wov5noHR8MNGefdPZApc2iIiIlEYJEREJIpsl7Za+j60HHDTOtP3f7tBjZpqPqdE8iDbIOEDiWRK25ePAj+JIbng0hYREYnCICEiIlEMDhKNRgMAyM3NxZtvvomysrIWz62oqIBSqURaWhqampruee0LFy4YWgYA4NixY5g7d+59vYeIiEzD4CCZNWsWTp8+DT8/P4wePRpJSUktnuvo6Ih169ahqKgI8+fP/9Pr1tfXY/r06YZX/AeCIKCxsfGB309EROIY/GH76NGjkZaWBl9fX2g0GtjZ2WH8+PFwdnZGXFwcfHx8ANy5G5kyZQo2btyImTNnQqvV/ul1c3NzMWDAAN3PaWlp6NSpE0JDQ3XHoqOjoVarYWNjAysrK1RXV6OyshKXL1+GIAjo0KEDUlJS7rd3IiIyAoOCpKysDCqVCvv27UNCQgKCg4MRFRUFDw8PFBQUYPr06cjKygIAXLx4UW/uekvz1JOTk3HixAkUFBTAxsYGI0eORHh4OF566SWMGzcOnp6e6N+/PwAgIyND773Hjh3Djh07MG/evAfpmYiIjOieQSIIApKTkzFo0CDEx8ejuroaUVFRutcrKyv1xt3m5+ejd+/eyM/Px6ZNm5CXlwcHBwdERERgyJAhuvNCQ0Pxl7/8BRMmTMDatWvRsWNHjBgxArGxsViwYAHmzp2LzMxMWFtb696zZ88erF69GhqNBv7+/sb6PSAyidb6vE5rrctc2L/x+79nkCgUCt0H25WVlYiIiMC0adMQHx+Py5cvo0+fPvj444915xcXFyM3NxdnzpzB+PHjMX36dNy4cQPx8fHo0aOHbgnMw8MDOTk5cHFxgYuLC4D/3r0MGDAAXl5eyMnJQUBAgO7aGRkZSE5Ohkqlwpo1a4z3u0BkAq1xTy+57zXG/lvBYKsuXbrAy8sL2dnZWLRoETp06NDsHHd3d/Ts2RNjxozRHevZsyd8fHygUql0QQIAX3zxBcaPHw8AUKvVencfixYt0vsZAMaMGYNXX30VTk5OsKDBjkREbdp9P0cSFhaGHTt23DVEAGDs2LF6IVJUVITk5GTk5uZi8ODBuuOnTp1CSUkJPD09UV9fj6NHj+LJJ5/Uvf7HEAGA4cOHIysrC71799a7FhERSee+t0gZMmQIli1bBo1Gc9e/7H/zzjvv4MyZM3B2dsbw4cORlpYGe3t73et79+7Fe++9h5KSEkycOBF1dXVYtWrVn/7a586dw+LFi+Hq6opJkybdb+lERGQCCuEB1oiqq6vh5OT0p+f89nVdY/r1119RWlqKJ554QvS1uNcWmdL25aNa5Vo8PyNg/5J/RvKbe4UIAKOHCAD06NEDPXr0MPp1iYjowXGvLSIiEkW228hzZjuZEuezk5zINkg4s12e5N4/kSlwaYuIiERhkBARkSiyXdrizHb54sx2IuOSbZDwORIyJc5sJznh0hYREYnCICEiIlEsJkgEQUBeXh7S09Oh0Whw+/ZtlJSU4OTJk0hPT8fZs2elLpGISJYs4jOSH3/8EXPmzEFpaSkSEhKwbt06fPXVV7Czs4OtrS1GjhyptyEkERGZj0Xckfj5+SEjIwO+vr4IDQ2FjY0NoqKi8PTTT6NTp04AgKNHj6KwsFDiSomI5Mci7kiOHDmCrKwsnDlzBh9++CGGDRuGzz//HHV1daisrERTUxMGDhyIXr16SV0qEZHsWESQBAcHAwAOHDiAhIQEnDt3Do8//jh69+6NY8eOITw8HKmpqdIWSfQHrfV5ndZal7mwfwlmtrcW+/btwyOPPIK33noLSqUSarUaTU1N0Gg0aGxslLo8omZa455ect9rjP23onkk5paTk4OOHTvC0dERCxcuxM8//4xNmzbh+PHjqKqqwrVr19C3b1+pyyQikiWL+LD90qVLiIuLAwB069YNjo6OePnll/HMM8/A29sbYWFhGDhwoMRVEhHJk0XckURGRur9/NBDD8Hb2xvW1tYoKSmBt7c37OzsJKqOiEjeLCJIfq+2thYzZ87UO7ZixQoAwLhx4zBs2DApyiIiki2LCpKUlBQAwPr166UthIiIdCziMxIiImq9LOqOxJg4s51MiTPbSU5kGySc2S5Pcu+fyBS4tEVERKIwSIiISBTZLm1xZrt8Gat/zmUnukO2QcKZ7SQW57IT3cGlLSIiEoVBQkREolhkkFy5cgWLFi2SugwiIoKFBsmhQ4fwyCOPSF0GERHBgj5s379/PxITE9GtWzfk5+fDy8sLS5YsgZeXFyoqKvC3v/0N0dHRUpdJRCQ7FnVHEhkZic2bN+Oxxx7T++eUKVOkLo2ISLYs5o6EqDWyxGdyLLFmY2L/Mp7ZTtQaWdq+XXLfa4z9m2Zmu0UtbRERUevDICEiIlEsYmlr06ZNSElJwa1bt7B3714UFBRAqVTq/llVVQVBEFBeXo5p06ZJXS4RkaxYRJBERkYiMjJS6jKIiOguuLRFRESiMEiIiEgUi1jaMgXObCexOJed6A7ZBglntsuT3PsnMgUubRERkSgMEiIiEkW2S1uc2S5fxuif89qJ/ku2QcKZ7SQG57UT/ReXtoiISBQGCRERiWKSINFoNACA3NxcvPnmmygrK7vreTdv3kRubi4OHz6M7du34/PPP0dRUVGL17116xaSkpJw9uxZU5RNREQPwCRBMmvWLJw+fRp+fn4YPXo0kpKS7nreoUOHsHDhQnz77be4cuUKkpKSkJ+f3+J1FQoFPD09sXr1akRERKC+vt4U5RMR0X0wyYfto0ePRlpaGnx9faHRaGBnZ4fx48fD2dkZcXFx8PHxAQCEhoYiNDQUAPDee+9h6tSpCA4ObvG69vb2CAsLQ1hYGFQqFRwcHExRPhER3QejB0lZWRlUKhX27duHhIQEBAcHIyoqCh4eHigoKMD06dORlZWl9561a9fCzs4OEyZMaPG6hYWF2Lt3L44cOYKKigoIgoCAgADMmTPH2C0QEdF9MGqQCIKA5ORkDBo0CPHx8aiurkZUVJTu9crKStjZ2em9Z9OmTdi2bRsCAwMRExMDrVaLcePGYcSIEXrnpaamws3NDR988AF69uwJrVaLCRMm4OTJkwgICDBmG0QGsdTncSy1bmNh/618ZrtCocDcuXMB3AmNiIgITJs2DfHx8bh8+TL69OmDjz/+WHd+eno6li1bpps34unpiYqKCsTFxcHT0xO+vr66cz/44AO9X8vKygo+Pj64ceOGMVsgMpgl7tkl973G2L9pZrab7IHELl26wMvLC9nZ2Vi0aBE6dOig9/qtW7dw5swZbNu2De7u7rrjzs7OGDhwIH755Re9ILmbS5cuYdSoUSapn4iIDGPS50jCwsKwY8eOZiECAO3bt8eSJUv0QuT27dvYtm0bjhw5gsDAwD+99t69e9HY2HjPsCEiItMy6RYpQ4YMwbJly6DRaGBtbX3Xc4qLi/Haa69BEATY29tj0KBB+OKLL9CpU6e7nn/+/Hl89dVXOH/+fItfKyYiIvMxaZDY29tj69atLYYIALi5uWHLli1/es7v7d+/HwEBAZg1axZsbW2NVSoRET0gk2/a6OTkdM9zDA0RAJg2bZqYcoiIyMi41xYREYki223kObOdxOC8dqL/km2QcGa7PMm9fyJT4NIWERGJwiAhIiJRZLu0xZnt8iWmf85qJ2pOtkHCme30IDirnag5Lm0REZEoDBIiIhLFIpa2CgsLMXPmTNTW1qKqqkq30WNeXp5u2iIAzJkzB35+flKVSUQkSxYRJL169UJGRga++uorVFRUYNKkSQAApVKJjIwMiasjIpI3i1ra+vHHH9G3b1+pyyAiot+xmCDRaDQ4ePAgPvnkE4wcORIrV67UvVZUVIQZM2agqalJwgqJiOTJIpa2ACArKwv9+/fHmjVrkJmZqdtCfseOHVi7di3ee+89bitPZmHpz+FYev1isf9WPrPdVOrr6/Gvf/0LPXv2RGZmJn744QfMnTsXX3zxBXJycpCenm7QdvVExmDJe3XJfa8x9m+ame0WsbT1yy+/4LXXXsPq1atx9OhRCIKAHj16AABmz57NECEikpBF3JH069cP/fr1Q0NDA3799VfMmDGj2TlbtmyBWq3GK6+8IkGFRETyZRF3JABw48YNvP766xg6dCgCAgIAADY2NqioqEBjYyMuXLggcYVERPJkEXcktbW1GD9+PCZPnozQ0FDd8cjISEyaNAlqtRpubm6YOHGihFUSEcmTRQRJx44dsW3bNlhZ6d9AhYeHIzw8XKKqiIgIsKClrT+GCBERtQ4WcUdiCpzZTg+Cs9qJmpNtkHBmuzzJvX8iU+B6ERERicIgISIiURgkREQkCoOEiIhEYZAQEZEoDBIiIhKFQUJERKLI9jkSKyuF1CVIRs69A+yf/bN/Y79PIQiC/J7KIyIio+HSFhERicIgISIiURgkREQkCoOEiIhEYZAQEZEoDBIiIhKFQUJERKIwSIiISBQGCRERicIgISJqIwoKCrB69Wo8/fTTOHnypNl+3TYbJPn5+YiJiYFSqURUVBTy8/ObnXP69GlERkZCqVRiwoQJKC0tlaBS0zCk/+zsbERFRSEqKgrjxo3DxYsXJajU+Azp/Tf79u2Dr68vVCqVGSs0LUP6r6+vx8yZMxEREQGlUomlS5dCq9VKUK3xGdL/zp07ER4ejujoaIwZMwbnzp2ToFLj+/rrr9GxY0e4urq2eM4PP/wApVKJUaNGIT4+HrW1teJ/YaEN0mg0wogRI4SDBw8KgiAIBw4cEEJDQ/XOqampEQYPHizk5eUJgiAI6enpwquvvmr2Wk3BkP4rKyuFoUOHCteuXRMEQRCys7OFl156yey1Gpshvf+msLBQGDt2rBAZGSkUFRWZs0yTMbT/+fPn687RarXCxYsXBa1Wa9ZaTcGQ/q9fvy48/fTTQllZmSAIgnD48GHhueeeM3utpjRu3DjhxIkTzY4XFRUJQUFBuj/3S5cuFebNmyf612uTdyTnz5+HnZ0dgoKCAADPPPMMrK2tceHCBd05hw8fxoABA9C3b18AQEREBM6dO4eqqipJajYmQ/q3t7dHUlISunfvDgDo06cPioqKJKnXmAzpHQBu376NGTNmYPbs2bC1tZWiVJMwpP/bt2/j1KlTKCoqwtixYzF58mS0a9cOCoXl74prSP81NTVwdXWFi4sLAODxxx+HIJO9a3fv3o2QkBDdn/uJEyciKytL9HXbZJBcvXoVjzzyiN6xRx99FFevXm3xHGtra7i7u7eJJQ5D+re3t4efnx8AoLa2FjNnzsSYMWPMWqcpGNI7AMydOxeRkZHw8fExZ3kmZ0j/xcXF+OWXX6BQKJCWlobY2FhMmjQJarXa3OUanSH99+7dGyEhIVixYgVycnLw2Wef4eOPPzZ3qZIoLCzU+/3p3LkzbG1tUVlZKeq6bTJIbGxsmv3XlUKh0DtmyDmW6n56u3DhAqKjo/HUU08hISHBXCWajCG9p6eno127dlAqleYuz+QM6b+urg6Ojo6Ijo4GADz11FNwcHDAlStXzFqrKRjSf1NTE+rq6vDEE0/A29sbDg4OyMnJMXepkrC1tb3r3wNi/95rk4OtevXqhYKCAr1jBQUF8PDw0P3s4eGh9y+PIAhQqVRwc3MzV5kmY0j/AHDixAksXrwYK1euxKOPPmrGCk3HkN6/++47lJSUICQkBABw7do1xMbG4o033kBYWJg5yzU6Q/p3d3dvtpRjZWUFa2trc5RoUob0v3XrVjQ2NiI4OBgA8Oqrr0KpVCI4OLjZ3Uxb4+Hhoff7U1tbC7Vajc6dO4u6bpu8I/Hx8UFTUxOOHj0KADh58iSampr0ljGCgoJw9uxZXLp0CQCwfft29O/fX/RvaGtgSP9VVVX48MMP8emnn7aZEAEM633Dhg3Ys2cPdu3ahV27dmHAgAFYv369xYcIYFj/zs7O8PPzw5YtWwAA54wJqWoAAAE9SURBVM6dQ319fZv498CQ/u3t7XH27Fk0NjYCAFQqFYqLi2Fj0yb/u1pPSEgIdu/ejZKSEgB3/iy8+OKLoq/bZickXrlyBbNnz4ZarYatrS3mz58PJycnvPHGG8jIyABw51+yBQsWwNraGk5OTvjoo4/QrVs3iSs3jnv1v3XrVixdurTZXcqKFSss/vfAkP/vfy8mJgYfffQR3N3dJajW+Azpv6SkBLNmzUJlZSXatWuHxMRE3RdPLN29+tdqtUhKSsL+/fthb28PtVqN2NhYjBw5UurSjSYmJgbx8fEICAhAbm4uPvnkEyQlJQEA9uzZg1WrVsHOzg5ubm5YuHAhOnbsKOrXa7NBQkRE5tEml7aIiMh8GCRERCQKg4SIiERhkBARkSgMEiIiEoVBQkREojBIiIhIFAYJERGJ8n8ey6jka2BDpwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTP5u5asL06S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}